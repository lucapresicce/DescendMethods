% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/SteepestDescend.R
\name{SteepD}
\alias{SteepD}
\title{Steepest Descend}
\usage{
SteepD(data, init = NULL, tol = 1e-04, maxit = 1000L, verb = F, check_loss = F)
}
\arguments{
\item{data}{\link{list} containing the data, elements must be named \code{X} and \code{Y}, where \code{X} is a
\code{n x k} matrix and \code{Y} is a vector of length \code{n}. Here, \code{n} represents the number of
observations and \code{k} is the number of  \mjseqn{\beta} coefficients.}

\item{init}{\link{vector} initial guesses of the parameter of interest. If \code{NULL}, values are all set equal to \code{1}.}

\item{tol}{\link{numeric} it must be strictly positive. It is the tolerance on the error evaluation between subsequent iterations. It is use to determine the stopping criteria.}

\item{maxit}{\link{integer} it must be strictly positive. It is the maximum number of iterations.}

\item{verb}{\link{bool} if \code{TRUE}, it prints more information about the status of the algorithm.}

\item{check_loss}{\link{bool} if \code{TRUE}, the algorithm stops when
\mjsdeqn{|| L(\beta(1) - L\beta(0))||\infty < tol}
otherwise, it stops when \mjsdeqn{||\beta(1) - \beta(0))||_\infty < tol}.}
}
\value{
\link{list} composed by
: \code{Beta_hat} the \mjseqn{\beta} coefficient of interest
: \code{Minimum}  the value of the loss function at the convergence point (only     if \code{verb = TRUE})
: \code{Final_error} the value of the error at the convergence point
: \code{Num_iter} the number of iterations that the function used to reach the     minimum
: \code{Time} it is the time elapsed to perform the optimization (increased by 2         seconds to make it traceable even with small data)
}
\description{
\loadmathjax Implements steepest descend method to find the coefficients \mjseqn{\beta} that minimize the following loss function
\mjsdeqn{L(\beta) = (X\beta - Y)^2}
In this implementation, the stepsize is updated at each iteration employing the gradient
\mjsdeqn{\nabla L(\beta) = 2X^{T}X\beta - 2X^{T}Y}
and the Hessian matrix
\mjsdeqn{H L(\beta) = 4X^{T}X}
}
