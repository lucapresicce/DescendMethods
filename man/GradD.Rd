% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/GradientDescend.R
\name{GradD}
\alias{GradD}
\title{Gradient Descend}
\usage{
GradD(
  data,
  stepsize = 1e-04,
  init = NULL,
  tol = 1e-04,
  maxit = 1000L,
  verb = F,
  check_loss = F
)
}
\arguments{
\item{data}{\link{list} containing the data, elements must be named \code{X} and \code{Y}, where \code{X} is a
\code{n x k} matrix and \code{Y} is a vector of length \code{n}. Here, \code{n} represents the number of
observations and \code{k} is the number of  \mjseqn{\beta} coefficients.}

\item{stepsize}{\link{numeric} it must be strictly positive. It is the step size, also called learning parameter. The suggested value is \code{0.1*(1/nrow(data$X))}}

\item{init}{\link{vector} initial guesses of the parameter of interest. If \code{NULL}, values are all set equal to \code{1}.}

\item{tol}{\link{numeric} it must be strictly positive. It is the tolerance on the error evaluation between subsequent iterations. It is use to determine the stopping criteria.}

\item{maxit}{\link{integer} it must be strictly positive. It is the maximum number of iterations.}

\item{verb}{\link{bool} if \code{TRUE}, it prints more information about the status of the algorithm (default is \code{FALSE}).}

\item{check_loss}{\link{bool} if \code{TRUE}, the algorithm stops when
\mjsdeqn{|| L(\beta(1) - L\beta(0))||\infty < tol}
otherwise, it stops when \mjsdeqn{||\beta(1) - \beta(0))||_\infty < tol}.}
}
\value{
\link{list} composed by
: \code{Beta_hat} the \mjseqn{\beta} coefficient of interest
: \code{Minimum}  the value of the loss function at the convergence point (only     if \code{verb = TRUE})
: \code{Final_error} the value of the error at the convergence point
: \code{Num_iter} the number of iterations that the function used to reach the     minimum
: \code{Time} it is the time elapsed to perform the optimization (increased by 2         seconds to make it traceable even with small data)
}
\description{
\loadmathjax Implements gradient descend method to find the coefficients \mjseqn{\beta} that minimize the following loss function
\mjsdeqn{L(\beta) = (X\beta - Y)^2}
In this implementation, the stepsize is kept constant. To reach the minimum, it employs the gradient
\mjsdeqn{\nabla L(\beta) = 2X^{T}X\beta - 2X^{T}Y}
}
