---
title: "DescendMethods"
author: "Luca Presicce, Alessandro Colombi"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{DescendMethods}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Contents of the package

In this package are included two principal functions: `GradD()` and `SteepD()`. These perform the algorithm of optimization called Gradient Descend and Steepest Descend, respectively. These functions are especially build to perform an optimization in order to find $\beta$ parameters of a liner model, i.e. 
$$
Y=X\beta + \varepsilon
$$

```{r setup}
library(DescendMethods)
```

## Question 1 - Implement a gradient descend function optimization

In order to answer this question, we have implemented the function called `GradD()`. This take as input the following argument (that are full explained in the function documentation, callable with `?GradD`) :
```{r, eval=FALSE}
GradD(data,
      stepsize = 1e-04,
      init = NULL,
      tol = 1e-04,
      maxit = 1000L,
      verb = F,
      check_loss = F)
```

and returns a list of element composed by: 
  : `Beta_hat` that contains the $\beta$ coefficient of interest
  : `Minimum` return the value of the loss function at the convergence point (only if `verb = TRUE`)
  : `Final_error` return the value of the error at the convergence point
  : `Num_iter` return the number of iterations that the function used to reach the minimum
  : `Time` it is the time elapsed to perform the optimization (increased by 2 seconds to make it traceable even with small data)

We want to notice that, since the stopping criteria consider only the absolute variation on the $\beta$ coefficient and we think that it can be misleading in certain cases, we have introduced a boolean flag called `check_loss = T`. This option, if `TRUE`, allow to evaluate the absolute difference from the loss function, instead on the parameters.

## Question 2 - Steepest descend method

In order to answer this question, we have implemented the function called `SteepD()`. This take as input the following argument (that are full explained in the function documentation, callable with `?SteepD`) :
```{r, eval=FALSE}
SteepD(data,
       init = NULL,
       tol = 1e-04,
       maxit = 1000L,
       verb = F,
       check_loss = F)
```

and return a list of element composed by: 
  : `Beta_hat` that contains the $\beta$ coefficient of interest
  : `Minimum` return the value of the loss function at the convergence point (only if `verb = TRUE`)
  : `Final_error` return the value of the error at the convergence point
  : `Num_iter` return the number of iterations that the function used to reach the minimum
  : `Time` it is the time elapsed to perform the optimization (increased by 2 seconds to make it traceable even with small data)

We want to notice that, since the stopping criteria consider only the absolute variation on the $\beta$ coefficient and we think that it can be misleading in certain cases, we have introduced a boolean flag called `check_loss = T`. This option, if `TRUE`, allow to evaluate the absolute difference from the loss function, instead on the parameters.


## Question 3 - Considerations

In order to answer this question, we need to make an example. We have choose to 
```{r}
# g <- GradD()
# s <- SteepD()
```


## Question 4 - Parallel cross-validation
The package also provides two functions that perform a cross validation on predictive capability for a linear model, achieved by a set of parameters obtained throught an optimization process. They are `CrossVD()` and `PcrossVD()`, and they were written following two different flow of operation: sequential and parallel respectively. 

The call for the sequential version is the following one:
```{r, eval=FALSE}
CrossVD(data, 
        K = NULL, 
        get_mean = T,
        OPT, ...)
```
As usual, all the arguments are all explained in the function documentation. We here just report that the idea is to build a function that takes the name of the optimization function to be used as an argument. At the moment, `OPT` accepts only `GradD` and `SteepD`. 
Moreover, if `get_mean` is `TRUE`, the CV-MSE is returned. Otherwise the function returns a vector containing all MSE computed for each fold.

The call for the parallel version is the following one:
```{r, eval=FALSE}
PcrossVD(data, 
        K = NULL, 
        get_mean = T,
        n_clust = NULL,
        OPT, ...)
```
The only difference with respect to the previous one is the additional argument to get the number of cores/clusters the user want to use.
In both functions, we rely on the map-reduce paradigm. In the parallel version, only the map transformation has been parallelized. We encounterd a problem for the parallel version,
indeed we always get an error message
**Aggiungere errore**

Apparently, the problem has been solved by calling the OPT function at least once before the parallelization. Namely, 
```{r, eval=FALSE}
OPT(df, maxit=1)
## 1 - define the cluster
cluster <- makeCluster(n_clust, type = "SOCK")
registerDoSNOW(cluster)
```
We are aware that this is not an elegant solution and it would be nice to understand what is happening to solve the problem.


## Examples 

Below are some code chunks as examples, in which the functions are implemented.

simulated data
```{r}
set.seed(1)
n <- 100
x <- cbind(1, matrix(runif(n*3, 0, 10), nrow = n, ncol = 3))
beta <- rnorm(4, 5, 5)
y <- x %*% beta + rnorm(n)
df <- list(X = x, Y = y)
res <- GradD(data = df, verb = F, maxit = 3000)
res$Beta_hat
```

```{r}
set.seed(1)
n <- 100
x <- cbind(1, matrix(runif(n*5, 0, 10), nrow = n, ncol = 5))
beta <- rnorm(6, 5, 5)
y <- x %*% beta + rnorm(n)
df <- list(X = x, Y = y)
res <- SteepD(data = df, verb = F)
res$Beta_hat
```



simulated data - loss evaluation
```{r}
set.seed(1)
n <- 100
x <- cbind(1, matrix(runif(n*3, 0, 10), nrow = n, ncol = 3))
beta <- rnorm(4, 5, 5)
y <- x %*% beta + rnorm(n)
df <- list(X = x, Y = y)
res <- GradD(data = df, verb = F, check_loss = T, maxit = 3000)
res$Beta_hat
```

```{r}
set.seed(1)
n <- 100
x <- cbind(1, matrix(runif(n*5, 0, 10), nrow = n, ncol = 5))
beta <- rnorm(6, 5, 5)
y <- x %*% beta + rnorm(n)
df <- list(X = x, Y = y)
res <- SteepD(data = df, check_loss = T, verb = F)
res$Beta_hat
```



real data
```{r}
x <- as.matrix(trees$Height)
y <- trees$Girth
df <- list(X = x, Y = y)
opt <- optim(par = rep(1, 1), fn = LossD, X = x, Y = y, method = c("BFGS"))
res <- GradD(data = df, stepsize = 1e-6, check_loss = T, maxit = 30000, verb = F)
opt$par
res$Beta_hat
```

```{r}
x <- as.matrix(trees$Height)
y <- trees$Girth
df <- list(X = x, Y = y)
opt <- optim(par = rep(1, 1), fn = LossD, X = x, Y = y, method = c("BFGS"))
res <- SteepD(data = df, check_loss = T, maxit = 30000, verb = T)
opt$par
res$Beta_hat
```



real data - multiple
```{r}
x <- as.matrix(trees[, -1])
y <- trees$Girth
df <- list(X = x, Y = y)
opt <- optim(par = rep(1, 2), fn = LossD, X = x, Y = y, method = c("BFGS"))
res <- GradD(data = df, check_loss = T,stepsize = 1e-8, maxit = 30000, verb = F)
opt$par
res$Beta_hat
```

```{r}
x <- as.matrix(trees[, -1])
y <- trees$Girth
df <- list(X = x, Y = y)
opt <- optim(par = rep(1, 2), fn = LossD, X = x, Y = y, method = c("BFGS"))
res <- SteepD(data = df, check_loss = T, verb = F)
opt$par
res$Beta_hat
```

