---
title: "DescendMethods"
author: "Luca Presicce, Alessandro Colombi"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{DescendMethods}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Contents of the package

In this package are included two principal functions: `GradD()` and `SteepD()`. These perform the algorithm of optimization called Gradient Descend and Steepest Descend, respectively. These functions are especially build to perform an optimization in order to find $\beta$ parameters of a liner model, i.e. 
$$
Y=X\beta + \varepsilon
$$

```{r setup}
library(DescendMethods)
```

## Question 1 - Implement a gradient descend function optimization

In order to answer this question, we have implemented the function called `GradD()`. This take as input the following argument (that are full explained in the function documentation, callable with `?GradD`) :
```{r, eval=FALSE}
GradD(data,
      stepsize = 1e-04,
      init = NULL,
      tol = 1e-04,
      maxit = 1000L,
      verb = F,
      check_loss = F)
```

and return a list of element composed by: 
  : `Beta_hat` that contains the $\beta$ coefficient of interest
  : `Minimum` return the value of the loss function at the convergence point (only     if `verb = TRUE`)
  : `Final_error` return the value of the error at the convergence point
  : `Num_iter` return the number of iterations that the function used to reach the     minimum
  : `Time` it is the time elapsed to perform the optimization (increased by 2         seconds to make it traceable even with small data)

## Examples 

Below are some code chunks as examples, in which the functions are implemented.

simulated data
```{r}
set.seed(1)
n <- 100
x <- cbind(1, matrix(runif(n*3, 0, 10), nrow = n, ncol = 3))
beta <- rnorm(4, 5, 5)
y <- x %*% beta + rnorm(n)
df <- list(X = x, Y = y)
res <- GradD(data = df, verb = F, maxit = 3000)
res$Beta_hat
```

```{r}
set.seed(1)
n <- 100
x <- cbind(1, matrix(runif(n*5, 0, 10), nrow = n, ncol = 5))
beta <- rnorm(6, 5, 5)
y <- x %*% beta + rnorm(n)
df <- list(X = x, Y = y)
res <- SteepD(data = df, verb = F)
res$Beta_hat
```



simulated data - loss evaluation
```{r}
set.seed(1)
n <- 100
x <- cbind(1, matrix(runif(n*3, 0, 10), nrow = n, ncol = 3))
beta <- rnorm(4, 5, 5)
y <- x %*% beta + rnorm(n)
df <- list(X = x, Y = y)
res <- GradD(data = df, verb = F, check_loss = T, maxit = 3000)
res$Beta_hat
```

```{r}
set.seed(1)
n <- 100
x <- cbind(1, matrix(runif(n*5, 0, 10), nrow = n, ncol = 5))
beta <- rnorm(6, 5, 5)
y <- x %*% beta + rnorm(n)
df <- list(X = x, Y = y)
res <- SteepD(data = df, check_loss = T, verb = F)
res$Beta_hat
```



real data
```{r}
x <- as.matrix(trees$Height)
y <- trees$Girth
df <- list(X = x, Y = y)
loss <- function(b, data = df) {
  X <- df$X
  Y <- df$Y
  t(X %*% b - Y) %*% (X %*% b - Y)
}
opt <- optim(par = rep(1, 1), fn = loss, data = df, method = c("BFGS"))
res <- GradD(data = df, stepsize = 1e-6, check_loss = T, maxit = 30000, verb = F)
opt$par
res$Beta_hat
```

```{r}
x <- as.matrix(trees$Height)
y <- trees$Girth
df <- list(X = x, Y = y)
loss <- function(b, data = df) {
  X <- df$X
  Y <- df$Y
  t(X %*% b - Y) %*% (X %*% b - Y)
}
opt <- optim(par = rep(1, 1), fn = loss, data = df, method = c("BFGS"))
res <- SteepD(data = df, check_loss = T, maxit = 30000, verb = T)
opt$par
res$Beta_hat
```



real data - multiple
```{r}
x <- as.matrix(trees[, -1])
y <- trees$Girth
df <- list(X = x, Y = y)
loss <- function(b, data = df) {
  X <- df$X
  Y <- df$Y
  t(X %*% b - Y) %*% (X %*% b - Y)
}
opt <- optim(par = rep(1, 2), fn = loss, data = df, method = c("BFGS"))
res <- GradD(data = df, check_loss = T,stepsize = 1e-8, maxit = 30000, verb = F)
opt$par
res$Beta_hat
```

```{r}
x <- as.matrix(trees[, -1])
y <- trees$Girth
df <- list(X = x, Y = y)
loss <- function(b, data = df) {
  X <- df$X
  Y <- df$Y
  t(X %*% b - Y) %*% (X %*% b - Y)
}
opt <- optim(par = rep(1, 2), fn = loss, data = df, method = c("BFGS"))
res <- SteepD(data = df, check_loss = T, verb = F)
opt$par
res$Beta_hat
```

