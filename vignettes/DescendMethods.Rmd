---
title: "DescendMethods"
author: "Luca Presicce, Alessandro Colombi"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{DescendMethods}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Contents of the package

In this package are included two principal functions: `GradD()` and `SteepD()`. These perform the algorithm of optimization called Gradient Descend and Steepest Descend, respectively. These functions are especially build to perform an optimization in order to find $\beta$ parameters of a liner model, i.e. 
$$
Y=X\beta + \varepsilon
$$

```{r setup}
library(DescendMethods)
```

## Examples 

Below are some code chunks as examples, in which the functions are implemented.

simulated data
```{r}
set.seed(1)
n <- 100
x <- cbind(1, matrix(runif(n*3, 0, 10), nrow = n, ncol = 3))
beta <- rnorm(4, 5, 5)
y <- x %*% beta + rnorm(n)
df <- list(X = x, Y = y)
res <- GradD(data = df, verb = F, maxit = 3000)
res$Beta_hat
```

```{r}
set.seed(1)
n <- 100
x <- cbind(1, matrix(runif(n*5, 0, 10), nrow = n, ncol = 5))
beta <- rnorm(6, 5, 5)
y <- x %*% beta + rnorm(n)
df <- list(X = x, Y = y)
res <- SteepD(data = df, verb = F)
res$Beta_hat
```



simulated data - loss evaluation
```{r}
set.seed(1)
n <- 100
x <- cbind(1, matrix(runif(n*3, 0, 10), nrow = n, ncol = 3))
beta <- rnorm(4, 5, 5)
y <- x %*% beta + rnorm(n)
df <- list(X = x, Y = y)
res <- GradD(data = df, verb = F, check_loss = T, maxit = 3000)
res$Beta_hat
```

```{r}
set.seed(1)
n <- 100
x <- cbind(1, matrix(runif(n*5, 0, 10), nrow = n, ncol = 5))
beta <- rnorm(6, 5, 5)
y <- x %*% beta + rnorm(n)
df <- list(X = x, Y = y)
res <- SteepD(data = df, check_loss = T, verb = F)
res$Beta_hat
```



real data
```{r}
x <- as.matrix(trees$Height)
y <- trees$Girth
df <- list(X = x, Y = y)
loss <- function(b, data = df) {
  X <- df$X
  Y <- df$Y
  t(X %*% b - Y) %*% (X %*% b - Y)
}
opt <- optim(par = rep(1, 1), fn = loss, data = df, method = c("BFGS"))
res <- GradD(data = df, stepsize = 1e-6, check_loss = T, maxit = 30000, verb = F)
opt$par
res$Beta_hat
```

```{r}
x <- as.matrix(trees$Height)
y <- trees$Girth
df <- list(X = x, Y = y)
loss <- function(b, data = df) {
  X <- df$X
  Y <- df$Y
  t(X %*% b - Y) %*% (X %*% b - Y)
}
opt <- optim(par = rep(1, 1), fn = loss, data = df, method = c("BFGS"))
res <- SteepD(data = df, check_loss = T, maxit = 30000, verb = T)
opt$par
res$Beta_hat
```



real data - multiple
```{r}
x <- as.matrix(trees[, -1])
y <- trees$Girth
df <- list(X = x, Y = y)
loss <- function(b, data = df) {
  X <- df$X
  Y <- df$Y
  t(X %*% b - Y) %*% (X %*% b - Y)
}
opt <- optim(par = rep(1, 2), fn = loss, data = df, method = c("BFGS"))
res <- GradD(data = df, check_loss = T,stepsize = 1e-8, maxit = 30000, verb = F)
opt$par
res$Beta_hat
```

```{r}
x <- as.matrix(trees[, -1])
y <- trees$Girth
df <- list(X = x, Y = y)
loss <- function(b, data = df) {
  X <- df$X
  Y <- df$Y
  t(X %*% b - Y) %*% (X %*% b - Y)
}
opt <- optim(par = rep(1, 2), fn = loss, data = df, method = c("BFGS"))
res <- SteepD(data = df, check_loss = T, verb = F)
opt$par
res$Beta_hat
```

